{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\redti\\anaconda3\\envs\\cvproject\\Lib\\site-packages\\fairscale\\experimental\\nn\\offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n",
      "c:\\Users\\redti\\anaconda3\\envs\\cvproject\\Lib\\site-packages\\fairscale\\experimental\\nn\\offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/12/19 00:45:19] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\redti/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\redti/.paddleocr/whl\\\\rec\\\\en\\\\en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='c:\\\\Users\\\\redti\\\\anaconda3\\\\envs\\\\cvproject\\\\Lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='C:\\\\Users\\\\redti/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, formula_algorithm='LaTeXOCR', formula_model_dir=None, formula_char_dict_path=None, formula_batch_num=1, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, formula=False, ocr=True, recovery=False, recovery_to_markdown=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
     ]
    }
   ],
   "source": [
    "from florence_utils import run_inference\n",
    "from llama_utils import call_llama,call_llama_combine\n",
    "from ram_utils import load_tags_from_json,get_tags_for_id\n",
    "from paddle_utililty import extract_text_from_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "# from PIL import Image\n",
    "# import requests\n",
    "# import torch\n",
    "\n",
    "# # Device setup\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # Load the model and processor\n",
    "# model_name = \"google/owlvit-base-patch16\"\n",
    "# processor = OwlViTProcessor.from_pretrained(model_name)\n",
    "# model = OwlViTForObjectDetection.from_pretrained(model_name).to(device).eval()\n",
    "\n",
    "# # Load an example image \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open(\"data/image/AMBER_1.jpg\").convert(\"RGB\")\n",
    "\n",
    "# # Define text queries\n",
    "# text_queries = [\n",
    "#             \"couple\",\n",
    "#             \"field\",\n",
    "#             \"hand\",\n",
    "#             \"grass\",\n",
    "#             \"grassy\",\n",
    "#             \"green\",\n",
    "#             \"hill\",\n",
    "#             \"hillside\",\n",
    "#             \"person\",\n",
    "#             \"lush\",\n",
    "#             \"man\",\n",
    "#             \"pasture\",\n",
    "#             \"walk\",\n",
    "#             \"woman\"\n",
    "#         ]\n",
    "\n",
    "# # Process inputs\n",
    "# inputs = processor(text=text_queries, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# # Inference\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# # Post-processing\n",
    "# target_sizes = torch.tensor([image.size[::-1]])  # (height, width)\n",
    "# results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n",
    "# result = results[0]\n",
    "\n",
    "\n",
    "# scores = result[\"scores\"]\n",
    "# labels = result[\"labels\"]\n",
    "# boxes = result[\"boxes\"]\n",
    "\n",
    "\n",
    "# threshold = 0.2\n",
    "# high_conf_indices = scores > threshold\n",
    "# filtered_boxes = boxes[high_conf_indices]\n",
    "# filtered_scores = scores[high_conf_indices]\n",
    "# filtered_labels = labels[high_conf_indices]\n",
    "\n",
    "# # Display detected objects\n",
    "# for label, box, score in zip(filtered_labels, filtered_boxes, filtered_scores):\n",
    "#     print(f\"Detected '{text_queries[label]}' with confidence {score.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response for image 1: A serene landscape unfolds, with a woman walking through a lush green field towards a distant group of people, set against a breathtaking backdrop of mountains visible from the top of a hill.\n",
      "Response for image 2: A man is peacefully floating on a canoe in the calm water, surrounded by lush green trees as the sun shines bright in the sky with a vibrant rainbow visible above.\n",
      "Response for image 3: A family is enjoying a beautiful day outdoors, with children playing on a lush green field and lawn while one of them throws a frisbee, surrounded by vibrant yellow flowers scattered throughout the grass.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "# Directory containing the images\n",
    "image_directory = \"data/image\"\n",
    "tags_json_file = \"data/tags.json\"\n",
    "tags_dict = load_tags_from_json(tags_json_file)\n",
    "output_data = []\n",
    "\n",
    "# Create a temporary directory name\n",
    "temp_dir = \"temp_slices\"\n",
    "\n",
    "for i in range(1, 1005):\n",
    "    try:\n",
    "        img_path = f\"{image_directory}/AMBER_{i}.jpg\"\n",
    "\n",
    "        # Open the image\n",
    "        with Image.open(img_path) as img:\n",
    "            width, height = img.size\n",
    "\n",
    "            # Compute mid points (if not evenly divisible, the right/bottom gets the extra pixel)\n",
    "            mid_w = width // 2\n",
    "            mid_h = height // 2\n",
    "\n",
    "            # Coordinates for cropping into 4 pieces\n",
    "            # Top-left\n",
    "            box1 = (0, 0, mid_w, mid_h)\n",
    "            # Top-right\n",
    "            box2 = (mid_w, 0, width, mid_h)\n",
    "            # Bottom-left\n",
    "            box3 = (0, mid_h, mid_w, height)\n",
    "            # Bottom-right\n",
    "            box4 = (mid_w, mid_h, width, height)\n",
    "\n",
    "            # Ensure temp directory is clean\n",
    "            if os.path.exists(temp_dir):\n",
    "                shutil.rmtree(temp_dir)\n",
    "            os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "            # Crop and save the four pieces\n",
    "            piece_paths = []\n",
    "            for idx, box in enumerate([box1, box2, box3, box4], start=1):\n",
    "                cropped_img = img.crop(box)\n",
    "                cropped_path = os.path.join(temp_dir, f\"slice_{idx}.jpg\")\n",
    "                cropped_img.save(cropped_path)\n",
    "                piece_paths.append(cropped_path)\n",
    "\n",
    "        # Generate captions for each piece\n",
    "        caption_list = []\n",
    "        for piece_path in piece_paths:\n",
    "            piece_caption = run_inference(piece_path, '<CAPTION>')\n",
    "            caption_list.append(piece_caption)\n",
    "\n",
    "        # Cleanup temp directory\n",
    "        shutil.rmtree(temp_dir)\n",
    "\n",
    "        # Combine captions using your llama function\n",
    "        final_response = call_llama_combine(caption_list)\n",
    "        \n",
    "        print(f\"Response for image {i}: {final_response}\")\n",
    "        output_data.append({\"id\": i, \"response\": final_response})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Issue on ID: {i}, Error: {e}\")\n",
    "\n",
    "# Save the results to a JSON file\n",
    "output_file = \"evaluation_results/amber_evaluation_results.json\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(output_data, file, indent=4)\n",
    "\n",
    "print(f\"Evaluation results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/12/19 00:13:35] ppocr DEBUG: dt_boxes num : 0, elapsed : 0.1988203525543213\n",
      "[2024/12/19 00:13:35] ppocr DEBUG: cls num  : 0, elapsed : 0\n",
      "[2024/12/19 00:13:35] ppocr DEBUG: rec_res num  : 0, elapsed : 0.0\n",
      "Response for image 1: A couple walks hand in hand through a lush, green pasture on a hillside, surrounded by vibrant grass that stretches towards the sky. The warm sunlight casts a serene ambiance over the idyllic scene, with two people strolling together in harmony with nature.\n",
      "[2024/12/19 00:14:11] ppocr DEBUG: dt_boxes num : 0, elapsed : 0.26758837699890137\n",
      "[2024/12/19 00:14:11] ppocr DEBUG: cls num  : 0, elapsed : 0\n",
      "[2024/12/19 00:14:11] ppocr DEBUG: rec_res num  : 0, elapsed : 0.0\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import json\n",
    "\n",
    "# # Directory containing the images\n",
    "# image_directory = \"data/image\"\n",
    "# tags_json_file = \"data/tags.json\"\n",
    "# tags_dict = load_tags_from_json(tags_json_file)\n",
    "# output_data = []\n",
    "\n",
    "# # Loop through each image file and perform the inference for each task\n",
    "# for i in range(1, 1005):\n",
    "#     try:\n",
    "#     # Construct the image path\n",
    "#         img_path =  f\"{image_directory}/AMBER_{i}.jpg\"\n",
    "\n",
    "#         # Run inference with each task type\n",
    "#         # image_tags = get_tags_for_id(i, tags_dict)\n",
    "#         # object_detection_info = run_inference(img_path, '<OD>')\n",
    "#         # ocr_info = extract_text_from_image(img_path)\n",
    "#         caption = run_inference(img_path, '<CAPTION>')\n",
    "        \n",
    "#         caption_list=\n",
    "\n",
    "#         final_response=call_llama_combine(caption_list)\n",
    "#         # Generate final response using call_llama\n",
    "#         # base64_image=convert_to_base64(img_path)\n",
    "#         # final_response = call_llama(ocr_info, object_detection_info,base64_image)\n",
    "#         # final_response = call_llama(ocr_info, object_detection_info,image_tags,caption)\n",
    "        \n",
    "#         print(f\"Response for image {i}: {final_response}\")\n",
    "#         # print(f\"Basic Response for image {i}: {basic_response}\")\n",
    "#         # Append the result for this image\n",
    "#         output_data.append({\"id\": i, \"response\": final_response})\n",
    "#     except:\n",
    "#         print(\"Issue on ID: \"+str(i))\n",
    "\n",
    "# # Save the results to a JSON file\n",
    "# output_file = \"evaluation_results/amber_evaluation_results.json\"\n",
    "# with open(output_file, \"w\") as file:\n",
    "#     json.dump(output_data, file, indent=4)\n",
    "\n",
    "# print(f\"Evaluation results saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
