{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\redti\\anaconda3\\envs\\cvproject\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\redti\\anaconda3\\envs\\cvproject\\Lib\\site-packages\\fairscale\\experimental\\nn\\offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n",
      "c:\\Users\\redti\\anaconda3\\envs\\cvproject\\Lib\\site-packages\\fairscale\\experimental\\nn\\offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025/01/19 20:51:52] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\redti/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\redti/.paddleocr/whl\\\\rec\\\\en\\\\en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='c:\\\\Users\\\\redti\\\\anaconda3\\\\envs\\\\cvproject\\\\Lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='C:\\\\Users\\\\redti/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, formula_algorithm='LaTeXOCR', formula_model_dir=None, formula_char_dict_path=None, formula_batch_num=1, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, formula=False, ocr=True, recovery=False, recovery_to_markdown=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
     ]
    }
   ],
   "source": [
    "from florence_utils import run_inference\n",
    "from llama_utils import call_llama,call_llama_combine\n",
    "from ram_utils import load_tags_from_json,get_tags_for_id\n",
    "from paddle_utililty import extract_text_from_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "# from PIL import Image\n",
    "# import requests\n",
    "# import torch\n",
    "\n",
    "# # Device setup\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # Load the model and processor\n",
    "# model_name = \"google/owlvit-base-patch16\"\n",
    "# processor = OwlViTProcessor.from_pretrained(model_name)\n",
    "# model = OwlViTForObjectDetection.from_pretrained(model_name).to(device).eval()\n",
    "\n",
    "# # Load an example image \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open(\"data/image/AMBER_1.jpg\").convert(\"RGB\")\n",
    "\n",
    "# # Define text queries\n",
    "# text_queries = [\n",
    "#             \"couple\",\n",
    "#             \"field\",\n",
    "#             \"hand\",\n",
    "#             \"grass\",\n",
    "#             \"grassy\",\n",
    "#             \"green\",\n",
    "#             \"hill\",\n",
    "#             \"hillside\",\n",
    "#             \"person\",\n",
    "#             \"lush\",\n",
    "#             \"man\",\n",
    "#             \"pasture\",\n",
    "#             \"walk\",\n",
    "#             \"woman\"\n",
    "#         ]\n",
    "\n",
    "# # Process inputs\n",
    "# inputs = processor(text=text_queries, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# # Inference\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# # Post-processing\n",
    "# target_sizes = torch.tensor([image.size[::-1]])  # (height, width)\n",
    "# results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n",
    "# result = results[0]\n",
    "\n",
    "\n",
    "# scores = result[\"scores\"]\n",
    "# labels = result[\"labels\"]\n",
    "# boxes = result[\"boxes\"]\n",
    "\n",
    "\n",
    "# threshold = 0.2\n",
    "# high_conf_indices = scores > threshold\n",
    "# filtered_boxes = boxes[high_conf_indices]\n",
    "# filtered_scores = scores[high_conf_indices]\n",
    "# filtered_labels = labels[high_conf_indices]\n",
    "\n",
    "# # Display detected objects\n",
    "# for label, box, score in zip(filtered_labels, filtered_boxes, filtered_scores):\n",
    "#     print(f\"Detected '{text_queries[label]}' with confidence {score.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response for image 1: A panoramic view of a lush green field, with a group of people walking through it in the foreground and a distant hillside with trees and mountains beyond.\n",
      "Response for image 2: A man standing on top of a roof next to a body of water is gazing out at a rainbow over the lake where a boat glides with trees behind it, as he paddles across the calm waters in his canoe with a life jacket on.\n",
      "Response for image 3: A small child running across a lush green field, surrounded by yellow flowers and playing with a frisbee, as seen from multiple angles showcasing the beauty of nature and joyful childhood moments.\n",
      "Response for image 4: A woman paddling a canoe on a serene lake with lush green fields, trees, and a boat visible in the background.\n",
      "Response for image 5: A little girl jumping in the air on a beach with her friends nearby, a body of water with a boat in the distance, a little girl in a bathing suit running into the water, and a beach with waves crashing on the sand.\n",
      "Response for image 6: A group of people are shown riding bikes down a lush green hillside, while sheep graze on a nearby grassy hillside with trees in the background. In the foreground, a man rides his bike down a winding road next to a yellow sign, and another person is also seen riding their bike down the same type of road. The overall scene depicts a serene landscape with natural beauty and outdoor activities.\n",
      "Response for image 7: A young boy standing on a beach, holding a bucket, with his father or another adult standing next to him, while an older woman and child relax nearby.\n",
      "Response for image 8: A little boy with a pacifier in his mouth is standing in a field of grass, surrounded by a white goose on top of a lush green field and another duck in the background.\n",
      "Response for image 9: Two men, one wearing a white shirt and another in a red shirt, are engaged in a soccer match on a lush green field, with both of them displaying their kicking skills.\n",
      "Response for image 10: A man with dreadlocks and a top hat is juggling three orange and black balls on a white surface, as he skillfully tosses them in the air while wearing sunglasses and a black hat.\n",
      "Response for image 11: A group of three dogs walking on a sandy beach with their shadows casted on the ground as they stroll towards the sunset, accompanied by two other dogs walking side by side nearby.\n",
      "Response for image 12: A young boy is laying on the grass with a football, enjoying a relaxing moment in the park.\n",
      "Response for image 13: A woman standing next to a brown horse in a field, with her long brown hair visible, as she smiles at the camera.\n",
      "Response for image 14: A woman standing on top of a beach next to a body of water, reflected by her image in the lake.\n",
      "Response for image 15: A man riding a snowboard down a sand dune, with a mountain visible in the background, as well as a person walking on the beach below.\n",
      "Response for image 16: A dog running on a beach near the ocean, with footprints in the sand and clouds in the sky, while a person walks across a street with a skateboard nearby.\n",
      "Response for image 17: A person riding a surfboard on top of a wave, as a man in a wet suit is seen standing on one in the ocean (Top Right), with another reaching for something in the water (Bottom Left) while an individual in the distance holds onto the board in the same body of water (Top Left and Bottom Right images combined).\n",
      "Response for image 18: Three men standing on top of a mountain with mountains in the background, two of them next to each other while one stands apart, another man is seen at a distance standing by a lake.\n",
      "Response for image 19: A serene sunset scene unfolds as two people walk hand in hand on a winding path near the ocean, with the vibrant yellow sky reflecting off the gentle waves. In the distance, a tree silhouetted against the sun's warm rays stands proudly, while another tree's branches stretch out to frame the sun itself, shining brightly through its leaves. On a nearby hill, two men take in the breathtaking view, their canes resting against them as they gaze out at the vast expanse of the ocean, where a stunning sunset is casting a golden glow over the water and a lone tree stands sentinel in the foreground.\n",
      "Response for image 20: A group of people, including women, are walking on a beach with their feet in the sand, enjoying time next to the ocean with surfboards in hand.\n",
      "Response for image 21: Two dogs playing with each other on the ground, one of which is also laying down together with another dog.\n",
      "Response for image 22: A woman in a black bikini running into the water, as she was seen on top of the beach next to a cliff, with the background featuring a large rock formation, while another woman is shown below her, also in a bikini, running towards the ocean's edge where someone else can be seen standing.\n",
      "Response for image 23: A woman and a man walking up a sand dune together.\n",
      "Response for image 24: A young boy standing in a grassy field, holding a white ball that he had just thrown with his friend who is shown in the top left corner playing catch with another white ball.\n",
      "Response for image 25: A car is driving through a river, surrounded by trees with a large pine tree visible in the forest nearby, amidst snow-covered surroundings.\n",
      "Response for image 26: A little girl in a red dress standing on a dirt road, with a deer walking down a path behind her, as a woman walks alongside, both wearing similar dresses.\n",
      "Response for image 27: A man riding a motorcycle down a winding road next to tall grass on a race track down a street.\n",
      "Response for image 28: A woman running down a road with her arms outstretched, passing through a field with trees and clouds in the sky, as she is surrounded by another woman in a blue jacket who is also running towards her, both on a paved road that intersects with a gravel road where a person walking in the opposite direction.\n",
      "Response for image 29: A man riding a red motorcycle down a street with a white line next to him, on a journey that began on a race track where he sped past a person riding another bike.\n",
      "Response for image 30: A young girl running through a field of flowers with her hair blowing gently, as seen from different angles, surrounded by women who are also enjoying nature's beauty.\n",
      "Response for image 31: A young boy running on a beach next to the ocean, with the blue sky above him as he plays in the waves crashing on the sand.\n",
      "Response for image 32: A dog running on a beach near the ocean with its mouth open and a cat, a beach scene with waves crashing on the sand.\n",
      "Response for image 33: A white dog walking on a leash next to a person riding a bike, with the cyclist having a bag attached to the back.\n",
      "Response for image 34: A young boy is seen kicking a soccer ball in different parts of a field, with one part showing him sitting on the ground wearing blue clothing and sunglasses, another part showing a person pointing at the same grassy field with a tree behind them, two other parts showcasing the boy running across the field and a person kicking a soccer ball respectively.\n",
      "Response for image 35: A couple of people standing on top of a cliff overlooking the ocean, with the man on the left looking out at the sunset over the ocean with clouds in the sky.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m caption_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m piece_path \u001b[38;5;129;01min\u001b[39;00m piece_paths:\n\u001b[1;32m---> 53\u001b[0m     piece_caption \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpiece_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<CAPTION>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     caption_list\u001b[38;5;241m.\u001b[39mappend(piece_caption)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Cleanup temp directory\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\redti\\Desktop\\CV_NLP\\florence_utils.py:16\u001b[0m, in \u001b[0;36mrun_inference\u001b[1;34m(img_path, prompt_task)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_inference\u001b[39m(img_path,prompt_task):\n\u001b[1;32m---> 16\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m processor(text\u001b[38;5;241m=\u001b[39mprompt_task, images\u001b[38;5;241m=\u001b[39mimage, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device, torch_dtype)\n\u001b[0;32m     19\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     20\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     21\u001b[0m         pixel_values\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\redti\\anaconda3\\envs\\cvproject\\Lib\\site-packages\\PIL\\Image.py:3247\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3244\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   3246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3247\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3248\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "# Directory containing the images\n",
    "image_directory = \"data/image\"\n",
    "tags_json_file = \"data/tags.json\"\n",
    "tags_dict = load_tags_from_json(tags_json_file)\n",
    "output_data = []\n",
    "\n",
    "# Create a temporary directory name\n",
    "temp_dir = \"temp_slices\"\n",
    "\n",
    "for i in range(1, 1005):\n",
    "    try:\n",
    "        img_path = f\"{image_directory}/AMBER_{i}.jpg\"\n",
    "\n",
    "        # Open the image\n",
    "        with Image.open(img_path) as img:\n",
    "            width, height = img.size\n",
    "\n",
    "            # Compute mid points (if not evenly divisible, the right/bottom gets the extra pixel)\n",
    "            mid_w = width // 2\n",
    "            mid_h = height // 2\n",
    "\n",
    "            # Coordinates for cropping into 4 pieces\n",
    "            # Top-left\n",
    "            box1 = (0, 0, mid_w, mid_h)\n",
    "            # Top-right\n",
    "            box2 = (mid_w, 0, width, mid_h)\n",
    "            # Bottom-left\n",
    "            box3 = (0, mid_h, mid_w, height)\n",
    "            # Bottom-right\n",
    "            box4 = (mid_w, mid_h, width, height)\n",
    "\n",
    "            # Ensure temp directory is clean\n",
    "            if os.path.exists(temp_dir):\n",
    "                shutil.rmtree(temp_dir)\n",
    "            os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "            # Crop and save the four pieces\n",
    "            piece_paths = []\n",
    "            for idx, box in enumerate([box1, box2, box3, box4], start=1):\n",
    "                cropped_img = img.crop(box)\n",
    "                cropped_path = os.path.join(temp_dir, f\"slice_{idx}.jpg\")\n",
    "                cropped_img.save(cropped_path)\n",
    "                piece_paths.append(cropped_path)\n",
    "\n",
    "        # Generate captions for each piece\n",
    "        caption_list = []\n",
    "        for piece_path in piece_paths:\n",
    "            piece_caption = run_inference(piece_path, '<CAPTION>')\n",
    "            caption_list.append(piece_caption)\n",
    "\n",
    "        # Cleanup temp directory\n",
    "        shutil.rmtree(temp_dir)\n",
    "        full_image_caption = run_inference(img_path, '<CAPTION>')\n",
    "        # Combine captions using your llama function\n",
    "        final_response = call_llama_combine(caption_list,full_image_caption)\n",
    "        \n",
    "        print(f\"Response for image {i}: {final_response}\")\n",
    "        output_data.append({\"id\": i, \"response\": final_response})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Issue on ID: {i}, Error: {e}\")\n",
    "\n",
    "# Save the results to a JSON file\n",
    "output_file = \"evaluation_results/amber_evaluation_results.json\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(output_data, file, indent=4)\n",
    "\n",
    "print(f\"Evaluation results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "\n",
    "# # Directory containing the images\n",
    "# image_directory = \"data/image\"\n",
    "# tags_json_file = \"data/tags.json\"\n",
    "# tags_dict = load_tags_from_json(tags_json_file)\n",
    "# output_data = []\n",
    "\n",
    "# # Loop through each image file and perform the inference for each task\n",
    "# for i in range(1, 1005):\n",
    "#     try:\n",
    "#     # Construct the image path\n",
    "#         img_path =  f\"{image_directory}/AMBER_{i}.jpg\"\n",
    "\n",
    "#         # Run inference with each task type\n",
    "#         # image_tags = get_tags_for_id(i, tags_dict)\n",
    "#         # object_detection_info = run_inference(img_path, '<OD>')\n",
    "#         # ocr_info = extract_text_from_image(img_path)\n",
    "#         caption = run_inference(img_path, '<CAPTION>')\n",
    "        \n",
    "#         caption_list=\n",
    "\n",
    "#         final_response=call_llama_combine(caption_list)\n",
    "#         # Generate final response using call_llama\n",
    "#         # base64_image=convert_to_base64(img_path)\n",
    "#         # final_response = call_llama(ocr_info, object_detection_info,base64_image)\n",
    "#         # final_response = call_llama(ocr_info, object_detection_info,image_tags,caption)\n",
    "        \n",
    "#         print(f\"Response for image {i}: {final_response}\")\n",
    "#         # print(f\"Basic Response for image {i}: {basic_response}\")\n",
    "#         # Append the result for this image\n",
    "#         output_data.append({\"id\": i, \"response\": final_response})\n",
    "#     except:\n",
    "#         print(\"Issue on ID: \"+str(i))\n",
    "\n",
    "# # Save the results to a JSON file\n",
    "# output_file = \"evaluation_results/amber_evaluation_results.json\"\n",
    "# with open(output_file, \"w\") as file:\n",
    "#     json.dump(output_data, file, indent=4)\n",
    "\n",
    "# print(f\"Evaluation results saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
